{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMILES+LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGS2DqF1ybEh",
        "colab_type": "code",
        "outputId": "0a77102e-cceb-4261-fa63-750e1c1e670e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Installing dependencies for RDKit\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-31 03:32:38--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71785000 (68M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>]  68.46M  99.7MB/s    in 0.7s    \n",
            "\n",
            "2019-12-31 03:32:43 (99.7 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [71785000/71785000]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.2.0=py37_0\n",
            "    - ca-certificates==2019.10.16=0\n",
            "    - certifi==2019.9.11=py37_0\n",
            "    - cffi==1.13.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.7.12=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.1=he6710b0_1\n",
            "    - openssl==1.1.1d=h7b6447c_3\n",
            "    - pip==19.3.1=py37_0\n",
            "    - pycosat==0.6.3=py37h14c3975_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.0.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.4=h265db76_1\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_0\n",
            "    - ruamel_yaml==0.15.46=py37h14c3975_0\n",
            "    - setuptools==41.4.0=py37_0\n",
            "    - six==1.12.0=py37_0\n",
            "    - sqlite==3.30.0=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.36.1=py_0\n",
            "    - urllib3==1.24.2=py37_0\n",
            "    - wheel==0.33.6=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.2.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2019.10.16-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.9.11-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.13.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.7.12-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.1-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_3\n",
            "  pip                pkgs/main/linux-64::pip-19.3.1-py37_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h14c3975_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.0.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.4-h265db76_1\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.46-py37h14c3975_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-41.4.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.12.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.30.0-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.36.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.24.2-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.33.6-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m22.921s\n",
            "user\t0m7.233s\n",
            "sys\t0m2.719s\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.70.0               |   py37h9de70de_1         337 KB  conda-forge\n",
            "    boost-cpp-1.70.0           |       h8e57a91_2        21.1 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2019.11.28 |       hecc5488_0         145 KB  conda-forge\n",
            "    cairo-1.16.0               |    hfb77d84_1002         1.5 MB  conda-forge\n",
            "    certifi-2019.11.28         |           py37_0         148 KB  conda-forge\n",
            "    conda-4.8.0                |           py37_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.0            |       he983fc9_1         884 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.58.3                |py37h6f030ca_1002         3.3 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_1        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      14_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      14_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_2         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1005         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      14_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.7          |       h5ec1e0e_6         7.6 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_0         343 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc3755c2_1         609 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       hee79883_0         1.3 MB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.17.3               |   py37h95a1406_0         5.1 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1d             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-0.25.3              |   py37hb3f55d8_0        11.4 MB  conda-forge\n",
            "    pcre-8.43                  |       he1b5a44_0         257 KB  conda-forge\n",
            "    pillow-6.2.1               |   py37h34e0f95_0         588 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.18.2             |   py37h438ddbb_0          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    pytz-2019.3                |             py_0         237 KB  conda-forge\n",
            "    rdkit-2019.09.2            |   py37hb31dc5d_0        23.8 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.4                 |       h3b9ef0a_1         989 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       108.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.70.0-py37h9de70de_1\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.70.0-h8e57a91_2\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hfb77d84_1002\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.0-he983fc9_1\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.58.3-py37h6f030ca_1002\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_1\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-14_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-14_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.3.0-hdf63c60_2\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1005\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-14_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.7-h5ec1e0e_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_0\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc3755c2_1\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-hee79883_0\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.8.3-he1b5a44_1001\n",
            "  numpy              conda-forge/linux-64::numpy-1.17.3-py37h95a1406_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-0.25.3-py37hb3f55d8_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.43-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-6.2.1-py37h34e0f95_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.18.2-py37h438ddbb_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  pytz               conda-forge/noarch::pytz-2019.3-py_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2019.09.2-py37hb31dc5d_0\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.4-h3b9ef0a_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2019.10.16~ --> conda-forge::ca-certificates-2019.11.28-hecc5488_0\n",
            "  certifi               pkgs/main::certifi-2019.9.11-py37_0 --> conda-forge::certifi-2019.11.28-py37_0\n",
            "  conda                      pkgs/main::conda-4.7.12-py37_0 --> conda-forge::conda-4.8.0-py37_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_3 --> conda-forge::openssl-1.1.1d-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m34.243s\n",
            "user\t0m29.001s\n",
            "sys\t0m3.251s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pRxIafZyb71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmnZPnjwycF_",
        "colab_type": "code",
        "outputId": "278ea9cf-79cc-4b7e-b047-04a05a5a5854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpAkavx7ycOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"/content/drive/My Drive/dsb2/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXaovKMOycMW",
        "colab_type": "code",
        "outputId": "a76108ed-6aeb-41dc-b21e-218bb0209865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "file_list = os.listdir(root)\n",
        "\n",
        "num_mols = len(file_list)\n",
        "print(num_mols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4i0rVZ4ycDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_xyz(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        num_atoms = int(file.readline())\n",
        "        properties = file.readline().split()[5:17] # only take the properties used in the experiments \n",
        "        properties = [num.replace(b'*^', b'e') for num in properties] \n",
        "        properties = [float(prop) for prop in properties]\n",
        "        atom_types = [0]*num_atoms\n",
        "        coords = np.array(np.zeros([num_atoms,3]))\n",
        "        for na in range(num_atoms):\n",
        "            coord_line = file.readline().split()\n",
        "            atom_types[na] = coord_line[0]\n",
        "            xyz_coords = coord_line[1:4]\n",
        "            xyz_coords = [num.replace(b'*^', b'e') for num in xyz_coords] \n",
        "            coords[na,:] = [float(num) for num in xyz_coords]  \n",
        "        vib_freqs = file.readline()\n",
        "        smiles = file.readline().split()[0]\n",
        "        inchis = file.readline()\n",
        "        \n",
        "    return smiles, properties, atom_types, coords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S80GbQpRycBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import concurrent.futures\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import as_completed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVklPXpJy91P",
        "colab_type": "code",
        "outputId": "2c45d753-a879-46c1-dea2-9af149884fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import time\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "N= mp.cpu_count()\n",
        "files = os.scandir(root)\n",
        "print (N)\n",
        "with mp.pool.ThreadPool(processes = 8) as p:\n",
        "        results= p.map(read_xyz, [root+file.name for file in files])\n",
        "      \n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "500.0141966342926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ipzGswyb_9",
        "colab_type": "code",
        "outputId": "608ee33e-4218-4a2c-d65c-2b622846a072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(results))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu_iAWpIyb5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def column(matrix, i):\n",
        "    return [row[i] for row in matrix]\n",
        "\n",
        "smiles =column(results,0)\n",
        "properties =column(results,1)\n",
        "atom_types =column(results,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9KFLq3Z-BnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwo9Atjh-HEL",
        "colab_type": "code",
        "outputId": "bde9b869-654d-4d9b-87f8-25760bea3789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "dtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32'),('Col4','int32'), ('Col5','float32'), ('Col6','float32'),('Col7','int32'), ('Col8','float32'), ('Col9','float32'),('Col10','int32'), ('Col11','float32'), ('Col12','float32'),('Col13','float32'), ('Col14','float32'),('Col15','float32'), ('Col16','float32'),('Col17','float32')]\n",
        "values = properties\n",
        "index = ['Row'+str(i) for i in range(1, len(values)+1)]\n",
        "\n",
        "df = pd.DataFrame(values, index=index)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Row1</th>\n",
              "      <td>0.3676</td>\n",
              "      <td>68.74</td>\n",
              "      <td>-0.2352</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.2417</td>\n",
              "      <td>814.7537</td>\n",
              "      <td>0.142229</td>\n",
              "      <td>-363.839405</td>\n",
              "      <td>-363.831976</td>\n",
              "      <td>-363.831032</td>\n",
              "      <td>-363.870471</td>\n",
              "      <td>29.280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row2</th>\n",
              "      <td>1.8866</td>\n",
              "      <td>64.52</td>\n",
              "      <td>-0.2485</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>0.2409</td>\n",
              "      <td>794.2784</td>\n",
              "      <td>0.129457</td>\n",
              "      <td>-383.721976</td>\n",
              "      <td>-383.714630</td>\n",
              "      <td>-383.713685</td>\n",
              "      <td>-383.753023</td>\n",
              "      <td>28.544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row3</th>\n",
              "      <td>1.7780</td>\n",
              "      <td>73.30</td>\n",
              "      <td>-0.2542</td>\n",
              "      <td>0.0838</td>\n",
              "      <td>0.3380</td>\n",
              "      <td>883.2429</td>\n",
              "      <td>0.177401</td>\n",
              "      <td>-349.011613</td>\n",
              "      <td>-349.003832</td>\n",
              "      <td>-349.002888</td>\n",
              "      <td>-349.042983</td>\n",
              "      <td>30.929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row4</th>\n",
              "      <td>1.2074</td>\n",
              "      <td>73.61</td>\n",
              "      <td>-0.2583</td>\n",
              "      <td>0.0729</td>\n",
              "      <td>0.3312</td>\n",
              "      <td>881.9664</td>\n",
              "      <td>0.177890</td>\n",
              "      <td>-349.011971</td>\n",
              "      <td>-349.004251</td>\n",
              "      <td>-349.003307</td>\n",
              "      <td>-349.043189</td>\n",
              "      <td>31.339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row5</th>\n",
              "      <td>2.1403</td>\n",
              "      <td>66.86</td>\n",
              "      <td>-0.2510</td>\n",
              "      <td>0.0868</td>\n",
              "      <td>0.3378</td>\n",
              "      <td>831.9533</td>\n",
              "      <td>0.152815</td>\n",
              "      <td>-384.933448</td>\n",
              "      <td>-384.925779</td>\n",
              "      <td>-384.924835</td>\n",
              "      <td>-384.964883</td>\n",
              "      <td>29.600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1       2       3   ...          8           9           10      11\n",
              "Row1  0.3676  68.74 -0.2352  0.0065  ... -363.831976 -363.831032 -363.870471  29.280\n",
              "Row2  1.8866  64.52 -0.2485 -0.0075  ... -383.714630 -383.713685 -383.753023  28.544\n",
              "Row3  1.7780  73.30 -0.2542  0.0838  ... -349.003832 -349.002888 -349.042983  30.929\n",
              "Row4  1.2074  73.61 -0.2583  0.0729  ... -349.004251 -349.003307 -349.043189  31.339\n",
              "Row5  2.1403  66.86 -0.2510  0.0868  ... -384.925779 -384.924835 -384.964883  29.600\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn58-bMS-J-G",
        "colab_type": "code",
        "outputId": "bf2982b2-2497-45da-891a-b88c31adb287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCEcZDax-QXj",
        "colab_type": "code",
        "outputId": "a4c9b258-81f9-4d6b-c212-6b41f5baa2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# normalize target values to have a mean of 0 and std of 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() \n",
        "data_scaled=scaler.fit_transform(df)\n",
        "df = pd.DataFrame(data_scaled , index=index)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Row1</th>\n",
              "      <td>-1.497174</td>\n",
              "      <td>0.376456</td>\n",
              "      <td>0.381924</td>\n",
              "      <td>-0.143865</td>\n",
              "      <td>-0.337901</td>\n",
              "      <td>-0.570023</td>\n",
              "      <td>0.300064</td>\n",
              "      <td>-0.077239</td>\n",
              "      <td>-0.077249</td>\n",
              "      <td>-0.077249</td>\n",
              "      <td>-0.077207</td>\n",
              "      <td>0.189786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row2</th>\n",
              "      <td>-0.487877</td>\n",
              "      <td>-0.094090</td>\n",
              "      <td>-0.147182</td>\n",
              "      <td>-0.438433</td>\n",
              "      <td>-0.354346</td>\n",
              "      <td>-0.652968</td>\n",
              "      <td>-0.079868</td>\n",
              "      <td>-0.534428</td>\n",
              "      <td>-0.534445</td>\n",
              "      <td>-0.534445</td>\n",
              "      <td>-0.534387</td>\n",
              "      <td>0.027847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row3</th>\n",
              "      <td>-0.560036</td>\n",
              "      <td>0.884913</td>\n",
              "      <td>-0.373942</td>\n",
              "      <td>1.482575</td>\n",
              "      <td>1.641695</td>\n",
              "      <td>-0.292575</td>\n",
              "      <td>1.346334</td>\n",
              "      <td>0.263719</td>\n",
              "      <td>0.263720</td>\n",
              "      <td>0.263720</td>\n",
              "      <td>0.263736</td>\n",
              "      <td>0.552609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row4</th>\n",
              "      <td>-0.939170</td>\n",
              "      <td>0.919479</td>\n",
              "      <td>-0.537050</td>\n",
              "      <td>1.253233</td>\n",
              "      <td>1.501910</td>\n",
              "      <td>-0.297746</td>\n",
              "      <td>1.360880</td>\n",
              "      <td>0.263710</td>\n",
              "      <td>0.263710</td>\n",
              "      <td>0.263710</td>\n",
              "      <td>0.263732</td>\n",
              "      <td>0.642819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row5</th>\n",
              "      <td>-0.319307</td>\n",
              "      <td>0.166829</td>\n",
              "      <td>-0.246638</td>\n",
              "      <td>1.545697</td>\n",
              "      <td>1.637584</td>\n",
              "      <td>-0.500348</td>\n",
              "      <td>0.614968</td>\n",
              "      <td>-0.562285</td>\n",
              "      <td>-0.562295</td>\n",
              "      <td>-0.562295</td>\n",
              "      <td>-0.562253</td>\n",
              "      <td>0.260194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "Row1 -1.497174  0.376456  0.381924  ... -0.077249 -0.077207  0.189786\n",
              "Row2 -0.487877 -0.094090 -0.147182  ... -0.534445 -0.534387  0.027847\n",
              "Row3 -0.560036  0.884913 -0.373942  ...  0.263720  0.263736  0.552609\n",
              "Row4 -0.939170  0.919479 -0.537050  ...  0.263710  0.263732  0.642819\n",
              "Row5 -0.319307  0.166829 -0.246638  ... -0.562295 -0.562253  0.260194\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atMo9hSozFpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "zinc_charset = [\n",
        "    ' ', '#', ')', '(', '+', '-', '/', '1', '3', '2', '5', '4', '7', '6', '8',\n",
        "    '=', '@', 'C', 'B', 'F', 'I', 'H', 'O', 'N', 'S', '[', ']', '\\\\', 'c', 'l',\n",
        "    'o', 'n', 'p', 's', 'r'\n",
        "]\n",
        "\n",
        "\n",
        "class OneHotFeaturizer():\n",
        "  \"\"\"\n",
        "  NOTE(LESWING) Not Thread Safe in initialization of charset\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, charset=zinc_charset, padlength=60):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    charset: obj:`list` of obj:`str`\n",
        "      Each string is length 1\n",
        "    padlength: int\n",
        "      length to pad the smile strings to\n",
        "    \"\"\"\n",
        "    self.charset = charset\n",
        "    self.pad_length = padlength\n",
        "\n",
        "  def featurize(self, mols, verbose=True, log_every_n=1000):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    mols: obj\n",
        "      List of rdkit Molecule Objects\n",
        "    verbose: bool\n",
        "      How much logging\n",
        "    log_every_n:\n",
        "      How often to log\n",
        "    Returns\n",
        "    -------\n",
        "    obj\n",
        "      numpy array of features\n",
        "    \"\"\"\n",
        "    from rdkit import Chem\n",
        "    smiles = [Chem.MolToSmiles(mol) for mol in mols]\n",
        "    if self.charset is None:\n",
        "      self.charset = self._create_charset(smiles)\n",
        "    return np.array([self.one_hot_encoded(smile) for smile in smiles])\n",
        "\n",
        "  def one_hot_array(self, i):\n",
        "    \"\"\"\n",
        "    Create a one hot array with bit i set to 1\n",
        "    Parameters\n",
        "    ----------\n",
        "    i: int\n",
        "      bit to set to 1\n",
        "    Returns\n",
        "    -------\n",
        "    obj:`list` of obj:`int`\n",
        "      length len(self.charset)\n",
        "    \"\"\"\n",
        "    return [int(x) for x in [ix == i for ix in range(len(self.charset))]]\n",
        "\n",
        "  def one_hot_index(self, c):\n",
        "    \"\"\"\n",
        "    TODO(LESWING) replace with map lookup vs linear scan\n",
        "    Parameters\n",
        "    ----------\n",
        "    c\n",
        "      character whose index we want\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "      index of c in self.charset\n",
        "    \"\"\"\n",
        "    return self.charset.index(c)\n",
        "\n",
        "  def pad_smile(self, smile):\n",
        "    \"\"\"\n",
        "    Pad A Smile String to self.pad_length\n",
        "    Parameters\n",
        "    ----------\n",
        "    smile: str\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "      smile string space padded to self.pad_length\n",
        "    \"\"\"\n",
        "\n",
        "    return smile.ljust(self.pad_length)\n",
        "\n",
        "  def one_hot_encoded(self, smile):\n",
        "    \"\"\"\n",
        "    One Hot Encode an entire SMILE string\n",
        "    Parameters\n",
        "    ----------\n",
        "    smile: str\n",
        "      smile string to encode\n",
        "    Returns\n",
        "    -------\n",
        "    object\n",
        "      np.array of one hot encoded arrays for each character in smile\n",
        "    \"\"\"\n",
        "    return np.array([\n",
        "        self.one_hot_array(self.one_hot_index(x)) for x in self.pad_smile(smile)\n",
        "    ])\n",
        "\n",
        "  def untransform(self, z):\n",
        "    \"\"\"\n",
        "    Convert from one hot representation back to SMILE\n",
        "    Parameters\n",
        "    ----------\n",
        "    z: obj:`list`\n",
        "      list of one hot encoded features\n",
        "    Returns\n",
        "    -------\n",
        "    Smile Strings picking MAX for each one hot encoded array\n",
        "    \"\"\"\n",
        "    z1 = []\n",
        "    for i in range(len(z)):\n",
        "      s = \"\"\n",
        "      for j in range(len(z[i])):\n",
        "        oh = np.argmax(z[i][j])\n",
        "        s += self.charset[oh]\n",
        "      z1.append([s.strip()])\n",
        "    return z1\n",
        "\n",
        "  def _create_charset(self, smiles):\n",
        "    \"\"\"\n",
        "    create the charset from smiles\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles: obj:`list` of obj:`str`\n",
        "      list of smile strings\n",
        "    Returns\n",
        "    -------\n",
        "    obj:`list` of obj:`str`\n",
        "      List of length one strings that are characters in smiles.  No duplicates\n",
        "    \"\"\"\n",
        "    s = set()\n",
        "    for smile in smiles:\n",
        "      for c in smile:\n",
        "        s.add(c)\n",
        "    return [' '] + sorted(list(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT0y4adYzkCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rdkit import Chem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoyq-HemznTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " mols = [Chem.MolFromSmiles(smile) for smile in smiles]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPjvgGCzptr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featurizer = OneHotFeaturizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sKgyCiH7kqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biggest_mol_size = max([len(atom_list) for atom_list in atom_types])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HIgYeBm7mwS",
        "colab_type": "code",
        "outputId": "e1ab4465-5fea-4a5a-b8da-0979f3f51c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "biggest_mol_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdLzMcYFhnAi",
        "colab_type": "code",
        "outputId": "73df8bf5-7141-41e8-f516-b5e8278e2930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "torch.manual_seed(2) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdd97d81610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RGG3LuIyTMT",
        "colab_type": "code",
        "outputId": "a37c1af6-1d1b-4808-f7b7-6e03238ca3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(mols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-RuX_IPXHQz",
        "colab_type": "code",
        "outputId": "8cda2366-47fe-4d99-b438-6187384a137d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "featurizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.OneHotFeaturizer at 0x7fdd9b7359b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M5pi1lLbVXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hots = featurizer.featurize(mols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQI8yD2ebaua",
        "colab_type": "code",
        "outputId": "a8ab54fa-a71f-4267-aa9f-bf2a374a9b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "one_hots.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13000, 60, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FK0og7j_mSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_mu=df[df.columns[0]]\n",
        "y=y_mu\n",
        "y_mu=torch.FloatTensor(y_mu)\n",
        "y=y.to_numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6bfsEdVitFh",
        "colab_type": "code",
        "outputId": "735b8db1-3385-4862-d46f-0746c7987aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05b7E06wi3Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=y.reshape((13000,1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duAGCEhg4qQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_list2=[]\n",
        "for i in range(13000):\n",
        "  data = [np.array(one_hots[i],dtype='float64'),np.array(m[i],dtype='float64')]\n",
        "  #print (data)\n",
        "  input_list2.append(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Kwfr0sxhTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=input_list2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-FnqxeQKpi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_collate(batch):\n",
        "    if len(batch[0]) == 2:\n",
        "        datas, targets = zip(*batch)\n",
        "    return datas, targets   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBbbGoVbWrTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def my_collate(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    target = [item[1] for item in batch]\n",
        "    #data= torch.FloatTensor(data)\n",
        "   # target = torch.FloatTensor(target)\n",
        "    return [data, target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PF_MH4FHcFo",
        "colab_type": "code",
        "outputId": "e59339ab-3d85-45b3-e7bc-15e20598a1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Split datasets.\n",
        "'''\n",
        "test_dataset = dataset[:960]\n",
        "val_dataset = dataset[960:1920]\n",
        "train_dataset =dataset[1920:7860]\n",
        "test_loader = DataLoader(test_dataset, batch_size=60, shuffle=False, num_workers=4,collate_fn=lambda x: x)\n",
        "val_loader = DataLoader(val_dataset, batch_size=60, shuffle=False, num_workers=4,collate_fn=lambda x: x)\n",
        "train_loader = DataLoader(train_dataset, batch_size=60, shuffle=True, num_workers=4,collate_fn=lambda x: x)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_dataset = dataset[:960]\\nval_dataset = dataset[960:1920]\\ntrain_dataset =dataset[1920:7860]\\ntest_loader = DataLoader(test_dataset, batch_size=60, shuffle=False, num_workers=4,collate_fn=lambda x: x)\\nval_loader = DataLoader(val_dataset, batch_size=60, shuffle=False, num_workers=4,collate_fn=lambda x: x)\\ntrain_loader = DataLoader(train_dataset, batch_size=60, shuffle=True, num_workers=4,collate_fn=lambda x: x)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XcdVGc-q4PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Split datasets.\n",
        "test_dataset = dataset[:960]\n",
        "val_dataset = dataset[960:1920]\n",
        "train_dataset =dataset[1920:7860]\n",
        "test_loader = DataLoader(test_dataset, batch_size=60, shuffle=False, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=60, shuffle=False, num_workers=4)\n",
        "train_loader = DataLoader(train_dataset, batch_size=60, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KXGtzCGq7_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDCV7pBLiPnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 35\n",
        "hidden_dim = 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIl5Sg6mn-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bME1rIYFaRf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim,batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "       \n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,dtype=torch.float64).requires_grad_().to(device)\n",
        "       \n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,dtype=torch.float64).requires_grad_().to(device)\n",
        "        # 28 time steps\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        \n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 28, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        # out.size() --> 100, 10\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvrWvgeVdR-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "model = LSTMModel(input_dim=35, hidden_dim=60, layer_dim=256, output_dim=1).double().to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                       factor=0.7, patience=5,\n",
        "                                                       min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlBb6uJGFKVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbe3aiMWNRwf",
        "colab_type": "code",
        "outputId": "62861737-87ec-406a-efc7-7eb8d5060444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# Train the model\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train(epoc):\n",
        "    itera=0\n",
        "    model.train()\n",
        "    loss_all=0\n",
        "    for data, target in train_loader:\n",
        "        data = data.view(-1, 60, 35).requires_grad_().to(device)\n",
        "        #print(data.size())\n",
        "        target=target.to(device)\n",
        "        #data, target= data.to(device),target.to(device)\n",
        "        #print(data.device)\n",
        "        #print(target.device)\n",
        "        optimizer.zero_grad() #clear gradients\n",
        "        outputs=model(data)\n",
        "        #print(\"The output\")\n",
        "        #print(outputs)\n",
        "        #print(\"The actual target\")\n",
        "        #print(target)\n",
        "        #print(\"The ouput\")\n",
        "        #print (outputs)\n",
        "        loss = F.mse_loss(outputs,target)\n",
        "        #print('Iter: {:03d}, Loss: {:.7f}'.format(itera, lr, loss)) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all+=loss.item() * len(train_loader.dataset)\n",
        "        itera=itera+1\n",
        "    return loss_all / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "# Test the model\n",
        "#with torch.no_grad():\n",
        "    error = 0\n",
        "    total = 0\n",
        "    for data, target in loader: \n",
        "         data = data.view(-1, 60, 35).to(device)\n",
        "         target=target.to(device)\n",
        "         #data, target= data.to(device),target.to(device)\n",
        "         outputs=model(data)\n",
        "         error+=(outputs- (target)).abs().sum().item()\n",
        "    return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "best_val_error = None\n",
        "for epoch in range(1, 301):\n",
        "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "    loss = train(epoch)\n",
        "    val_error = test(val_loader)\n",
        "    scheduler.step(val_error)\n",
        "\n",
        "    if best_val_error is None or val_error <= best_val_error:\n",
        "        test_error = test(test_loader)\n",
        "        best_val_error = val_error\n",
        "\n",
        "    print('Epoch: {:03d}, LR: {:7f}, Loss: {:.7f}, Validation MAE: {:.7f}, '\n",
        "          'Test MAE: {:.7f}'.format(epoch, lr, loss, val_error, test_error))    \n",
        "    #print('Test Accuracy of the model on the test smiles : {} %'.format(100 * correct / total)) \n",
        "\n",
        "# Save the model checkpoint\n",
        "#torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, LR: 0.001000, Loss: 104.9837874, Validation MAE: 0.7324750, Test MAE: 0.7665570\n",
            "Epoch: 002, LR: 0.001000, Loss: 104.9842255, Validation MAE: 0.7309449, Test MAE: 0.7599377\n",
            "Epoch: 003, LR: 0.001000, Loss: 104.9805719, Validation MAE: 0.7319587, Test MAE: 0.7599377\n",
            "Epoch: 004, LR: 0.001000, Loss: 104.9227781, Validation MAE: 0.7334864, Test MAE: 0.7599377\n",
            "Epoch: 005, LR: 0.001000, Loss: 104.9202262, Validation MAE: 0.7329807, Test MAE: 0.7599377\n",
            "Epoch: 006, LR: 0.001000, Loss: 104.9296132, Validation MAE: 0.7315935, Test MAE: 0.7599377\n",
            "Epoch: 007, LR: 0.001000, Loss: 104.8987520, Validation MAE: 0.7324200, Test MAE: 0.7599377\n",
            "Epoch: 008, LR: 0.001000, Loss: 104.9233265, Validation MAE: 0.7334887, Test MAE: 0.7599377\n",
            "Epoch: 009, LR: 0.000700, Loss: 104.9101538, Validation MAE: 0.7326007, Test MAE: 0.7599377\n",
            "Epoch: 010, LR: 0.000700, Loss: 104.9046579, Validation MAE: 0.7326175, Test MAE: 0.7599377\n",
            "Epoch: 011, LR: 0.000700, Loss: 104.9046129, Validation MAE: 0.7323981, Test MAE: 0.7599377\n",
            "Epoch: 012, LR: 0.000700, Loss: 104.9411602, Validation MAE: 0.7318680, Test MAE: 0.7599377\n",
            "Epoch: 013, LR: 0.000700, Loss: 104.9089450, Validation MAE: 0.7320057, Test MAE: 0.7599377\n",
            "Epoch: 014, LR: 0.000700, Loss: 104.9214628, Validation MAE: 0.7322672, Test MAE: 0.7599377\n",
            "Epoch: 015, LR: 0.000490, Loss: 104.9015456, Validation MAE: 0.7326491, Test MAE: 0.7599377\n",
            "Epoch: 016, LR: 0.000490, Loss: 104.8936194, Validation MAE: 0.7322894, Test MAE: 0.7599377\n",
            "Epoch: 017, LR: 0.000490, Loss: 104.8933834, Validation MAE: 0.7325242, Test MAE: 0.7599377\n",
            "Epoch: 018, LR: 0.000490, Loss: 104.8974948, Validation MAE: 0.7320715, Test MAE: 0.7599377\n",
            "Epoch: 019, LR: 0.000490, Loss: 104.8965959, Validation MAE: 0.7323362, Test MAE: 0.7599377\n",
            "Epoch: 020, LR: 0.000490, Loss: 104.8973075, Validation MAE: 0.7324933, Test MAE: 0.7599377\n",
            "Epoch: 021, LR: 0.000343, Loss: 104.8945070, Validation MAE: 0.7324793, Test MAE: 0.7599377\n",
            "Epoch: 022, LR: 0.000343, Loss: 104.8896430, Validation MAE: 0.7324216, Test MAE: 0.7599377\n",
            "Epoch: 023, LR: 0.000343, Loss: 104.8928235, Validation MAE: 0.7324843, Test MAE: 0.7599377\n",
            "Epoch: 024, LR: 0.000343, Loss: 104.8920096, Validation MAE: 0.7322869, Test MAE: 0.7599377\n",
            "Epoch: 025, LR: 0.000343, Loss: 104.8995467, Validation MAE: 0.7324838, Test MAE: 0.7599377\n",
            "Epoch: 026, LR: 0.000343, Loss: 104.8894715, Validation MAE: 0.7324412, Test MAE: 0.7599377\n",
            "Epoch: 027, LR: 0.000240, Loss: 104.8855406, Validation MAE: 0.7323793, Test MAE: 0.7599377\n",
            "Epoch: 028, LR: 0.000240, Loss: 104.8969155, Validation MAE: 0.7323897, Test MAE: 0.7599377\n",
            "Epoch: 029, LR: 0.000240, Loss: 104.8947351, Validation MAE: 0.7325422, Test MAE: 0.7599377\n",
            "Epoch: 030, LR: 0.000240, Loss: 104.8891144, Validation MAE: 0.7323302, Test MAE: 0.7599377\n",
            "Epoch: 031, LR: 0.000240, Loss: 104.8977100, Validation MAE: 0.7324222, Test MAE: 0.7599377\n",
            "Epoch: 032, LR: 0.000240, Loss: 104.8943893, Validation MAE: 0.7323522, Test MAE: 0.7599377\n",
            "Epoch: 033, LR: 0.000168, Loss: 104.8850126, Validation MAE: 0.7324457, Test MAE: 0.7599377\n",
            "Epoch: 034, LR: 0.000168, Loss: 104.8892052, Validation MAE: 0.7324031, Test MAE: 0.7599377\n",
            "Epoch: 035, LR: 0.000168, Loss: 104.8907806, Validation MAE: 0.7324988, Test MAE: 0.7599377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4CvZB1YIQH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}